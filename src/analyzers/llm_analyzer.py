"""LLM-based project analyzer."""

from dataclasses import dataclass
from typing import Optional, Union

from openai import OpenAI
import httpx

from ..config import AnalyzerConfig
from ..collectors.github import GitHubProject
from ..collectors.hackernews import HNStory


@dataclass
class ProjectAnalysis:
    """Analysis result for a project."""
    title: str
    url: str
    source: str  # "github" or "hackernews"
    summary: str
    highlights: list[str]  # Key innovation points
    tech_stack: list[str]
    target_audience: str
    potential: str  # Growth potential assessment
    raw_data: dict  # Original project data


class LLMAnalyzer:
    """Analyzes projects using LLM."""
    
    SYSTEM_PROMPT = """‰Ω†ÊòØ‰∏Ä‰∏™ÊäÄÊúØË∂ãÂäøÂàÜÊûê‰∏ìÂÆ∂„ÄÇ‰Ω†ÁöÑ‰ªªÂä°ÊòØÂàÜÊûêÂºÄÊ∫êÈ°πÁõÆÔºåËØÜÂà´ÂÖ∂ÁúüÊ≠£ÁöÑ‰ª∑ÂÄº„ÄÇ
‰∏çË¶ÅÂè™ÊòØÁøªËØë ReadmeÔºåË¶ÅÊÄùËÄÉÔºöËøô‰∏™È°πÁõÆËß£ÂÜ≥‰∫Ü‰ªÄ‰πàÊ†∏ÂøÉÈóÆÈ¢òÔºüÂíåÁé∞ÊúâÊñπÊ°àÊØîÊúâ‰ªÄ‰πà‰∏çÂêåÔºü

ËØ∑Áî®ÁÆÄÊ¥Å„ÄÅ‰∏ì‰∏öÁöÑ‰∏≠ÊñáÂõûÂ§çÔºåÊ†ºÂºèÂ¶Ç‰∏ãÔºö

## ÊëòË¶Å
[1-2Âè•ËØùÊèèËø∞Ê†∏ÂøÉÂäüËÉΩÔºåÂº∫Ë∞É"Ëß£ÂÜ≥‰∫Ü‰ªÄ‰πàÁóõÁÇπ"]

## Ê†∏ÂøÉ‰∫ÆÁÇπ
- [ÂàõÊñ∞ÁÇπ (Â¶ÇÔºöÊØîXÂø´10ÂÄçÔºåÊàñÊîØÊåÅYÁâπÊÄß)]
- [ÊäÄÊúØ‰ºòÂäø]
- [Â∫îÁî®Âú∫ÊôØ]

## ÊäÄÊúØÊ†à
[‰∏ªË¶ÅËØ≠Ë®Ä/Ê°ÜÊû∂]

## Á´ûÂìÅÂØπÊØî
[‰∏ÄÂè•ËØùÂØπÊØîÂêåÁ±ªÈ°πÁõÆ (Â¶ÇÔºöÁ±ª‰ºº Lodash ‰ΩÜÊõ¥ËΩªÈáè)]

## ÈÄÇÂêà‰∫∫Áæ§
[Ë∞ÅÊúÄÈúÄË¶ÅÂÆÉÔºü]

## ÂèëÂ±ïÊΩúÂäõ
[ÁÆÄÁü≠ËØÑ‰º∞ÔºöÊòØÁé©ÂÖ∑È°πÁõÆËøòÊòØÁîü‰∫ßÁ∫ßÁ•ûÂô®Ôºü]
"""

    def __init__(self, config: AnalyzerConfig):
        self.config = config
        self.client = None
        if config.enabled and config.api_key:
            self.client = OpenAI(
                api_key=config.api_key,
                base_url=config.api_base if config.api_base else None,
                http_client=httpx.Client(http2=True),
            )
            # print(f"DEBUG: Initialized OpenAI Client with Base URL: {self.client.base_url}")
    
    def analyze_image(self, prompt: str, image_base64: str) -> str:
        """Analyze an image using the configured LLM model."""
        if not self.client:
            return "‚ùå AI Agent not configured"
            
        try:
            # Use configured model (GLM-4.7 supports multimodal according to docs)
            response = self.client.chat.completions.create(
                model=self.config.model,  # Use GLM-4.7 from config
                messages=[
                    {
                        "role": "user",
                        "content": [
                            {"type": "text", "text": prompt or "ËØ∑ÂàÜÊûêËøôÂº†ÂõæÁâáÔºåÂëäËØâÊàëÂõæÁâá‰∏≠ÁöÑÂÜÖÂÆπ"},
                            {
                                "type": "image_url",
                                "image_url": {
                                    "url": f"data:image/jpeg;base64,{image_base64}"
                                }
                            }
                        ]
                    }
                ],
                temperature=0.6,
                max_tokens=2048,
            )
            return response.choices[0].message.content
        except Exception as e:
            print(f"‚ùå Vision API Error: {e}")
            return f"ÂõæÁâáÂàÜÊûêÂ§±Ë¥•: {str(e)}"

    def analyze(
        self, 
        projects: list[Union[GitHubProject, HNStory]]
    ) -> list[ProjectAnalysis]:
        """Analyze a list of projects."""
        if not self.config.enabled or not self.client:
            # Return basic analysis without LLM
            return [self._basic_analysis(p) for p in projects]
        
        results = []
        for project in projects:
            try:
                analysis = self._analyze_single(project)
                results.append(analysis)
            except Exception as e:
                print(f"Error analyzing project: {e}")
                results.append(self._basic_analysis(project))
        
        return results
    
    def _analyze_single(
        self, 
        project: Union[GitHubProject, HNStory]
    ) -> ProjectAnalysis:
        """Analyze a single project using LLM."""
        # Build prompt based on project type
        if isinstance(project, GitHubProject):
            user_prompt = self._build_github_prompt(project)
            source = "github"
            title = project.name
            url = project.url
            raw_data = {
                "name": project.name,
                "description": project.description,
                "language": project.language,
                "stars": project.stars,
                "stars_today": project.stars_today,
            }
        else:
            user_prompt = self._build_hn_prompt(project)
            source = "hackernews"
            title = project.title
            url = project.url or project.hn_url
            raw_data = {
                "title": project.title,
                "score": project.score,
                "comments": project.comments,
            }
        
        # Call LLM
        try:
            response = self.client.chat.completions.create(
                model=self.config.model,
                messages=[
                    {"role": "system", "content": self.SYSTEM_PROMPT},
                    {"role": "user", "content": user_prompt},
                ],
                temperature=0.7,
                max_tokens=1024,
            )
        except Exception as api_error:
            print(f"  ‚ùå LLM API Error for {title}: {api_error}")
            raise
        
        # Debug: Print response object details
        content = response.choices[0].message.content if response.choices else ""
        if not content or len(content.strip()) < 10:
            print(f"  ‚ö†Ô∏è LLM returned empty for {title}, using fallback")
            return self._basic_analysis(project)
        
        # Parse response
        analysis = self._parse_llm_response(content or "")
        
        return ProjectAnalysis(
            title=title,
            url=url,
            source=source,
            summary=analysis.get("summary", ""),
            highlights=analysis.get("highlights", []),
            tech_stack=analysis.get("tech_stack", []),
            target_audience=analysis.get("target_audience", ""),
            potential=analysis.get("potential", ""),
            raw_data=raw_data,
        )
    
    def _build_github_prompt(self, project: GitHubProject) -> str:
        """Build prompt for GitHub project with README context."""
        readme_snippet = project.readme_content or "Êó†ËØ¶ÁªÜËØ¥Êòé"
        
        return f"""ËØ∑Ê∑±Â∫¶ÂàÜÊûêËøô‰∏™ GitHub È°πÁõÆÔºö

È°πÁõÆÂêçÁß∞Ôºö{project.name}
È°πÁõÆÂú∞ÂùÄÔºö{project.url}
ÊèèËø∞Ôºö{project.description or 'Êó†'}
ÁºñÁ®ãËØ≠Ë®ÄÔºö{project.language or 'Êú™Áü•'}
Star Êï∞Ôºö{project.stars:,}
‰ªäÊó•Êñ∞Â¢ûÔºö{project.stars_today:,}

‰ª•‰∏ãÊòØ README ÁöÑÂâç 3000 ‰∏™Â≠óÁ¨¶Ôºö
---
{readme_snippet}
---

ËØ∑ÂøΩÁï• README ‰∏≠ÁöÑÂÆâË£ÖÊ≠•È™§„ÄÅË¥°ÁåÆÊåáÂçóÁ≠âÊó†ÂÖ≥‰ø°ÊÅØÔºåÈáçÁÇπÊåñÊéòÔºöÊ†∏ÂøÉÂäüËÉΩ„ÄÅÊäÄÊúØ‰∫ÆÁÇπ„ÄÅËß£ÂÜ≥ÁöÑÁóõÁÇπ„ÄÇ
Â¶ÇÊûú README ÂÜÖÂÆπÂ§™Â∞ëÊàñÊó†ÂÖ≥ÔºåËØ∑Ê†πÊçÆÊèèËø∞Â∞ΩÂäõÂàÜÊûê„ÄÇ
"""

    def _build_hn_prompt(self, story: HNStory) -> str:
        """Build prompt for Hacker News story."""
        return f"""ËØ∑ÂàÜÊûêËøô‰∏™ Hacker News ÁÉ≠Èó®ÂÜÖÂÆπÔºö

Ê†áÈ¢òÔºö{story.title}
ÈìæÊé•Ôºö{story.url or '(Ask HN / Show HN)'}
ÂæóÂàÜÔºö{story.score}
ËØÑËÆ∫Êï∞Ôºö{story.comments}
HN ËÆ®ËÆ∫Ôºö{story.hn_url}
"""

    def _parse_llm_response(self, content: str) -> dict:
        """Parse LLM response into structured data."""
        result = {
            "summary": "",
            "highlights": [],
            "tech_stack": [],
            "competitors": "",  # New field
            "target_audience": "",
            "potential": "",
        }
        
        current_section = None
        lines = content.strip().split("\n")
        
        for line in lines:
            line = line.strip()
            if not line:
                continue
            
            if line.startswith("## ÊëòË¶Å"):
                current_section = "summary"
            elif line.startswith("## Ê†∏ÂøÉ‰∫ÆÁÇπ") or line.startswith("## ‰∫ÆÁÇπ"):
                current_section = "highlights"
            elif line.startswith("## ÊäÄÊúØÊ†à"):
                current_section = "tech_stack"
            elif line.startswith("## Á´ûÂìÅÂØπÊØî"):
                current_section = "competitors"
            elif line.startswith("## ÈÄÇÂêà‰∫∫Áæ§"):
                current_section = "target_audience"
            elif line.startswith("## ÂèëÂ±ïÊΩúÂäõ"):
                current_section = "potential"
            elif current_section:
                if current_section == "highlights":
                    if line.startswith("- "):
                        result["highlights"].append(line[2:])
                elif current_section == "tech_stack":
                    # Split by comma
                    techs = [t.strip() for t in line.split(",") if t.strip()]
                    result["tech_stack"].extend(techs)
                elif current_section in ("summary", "competitors", "target_audience", "potential"):
                    if current_section == "competitors" and result["competitors"]:
                         result["competitors"] += " " + line
                    elif result[current_section]:
                        result[current_section] += " " + line
                    else:
                        result[current_section] = line
        
        return result
    
    def _basic_analysis(
        self, 
        project: Union[GitHubProject, HNStory]
    ) -> ProjectAnalysis:
        """Create basic analysis without LLM - try to generate Chinese summary."""
        if isinstance(project, GitHubProject):
            # Try to generate a simple Chinese summary
            summary = self._generate_basic_chinese_summary(project)
            
            # Infer potential based on stars
            if project.stars >= 10000:
                potential = "üåü ÊàêÁÜüÈ°πÁõÆÔºåÁ§æÂå∫Ê¥ªË∑É"
            elif project.stars >= 1000:
                potential = "üìà Âø´ÈÄüÊàêÈïø‰∏≠"
            elif project.stars_today >= 100:
                potential = "üî• Êñ∞ÊòüÈ°πÁõÆÔºåÂÄºÂæóÂÖ≥Ê≥®"
            else:
                potential = "üå± Êó©ÊúüÈ°πÁõÆ"
            
            # Infer audience based on language
            lang = project.language or ""
            if lang.lower() in ["python", "jupyter notebook"]:
                audience = "AI/Êï∞ÊçÆÂºÄÂèëËÄÖ"
            elif lang.lower() in ["typescript", "javascript"]:
                audience = "ÂâçÁ´Ø/ÂÖ®Ê†àÂºÄÂèëËÄÖ"
            elif lang.lower() in ["go", "rust"]:
                audience = "ÂêéÁ´Ø/Âü∫Á°ÄËÆæÊñΩÂºÄÂèëËÄÖ"
            else:
                audience = "ÂºÄÂèëËÄÖ"
            
            return ProjectAnalysis(
                title=project.name,
                url=project.url,
                source="github",
                summary=summary,
                highlights=[f"‚≠ê {project.stars:,} Stars", f"üìà ‰ªäÊó• +{project.stars_today}"],
                tech_stack=[project.language] if project.language else [],
                target_audience=audience,
                potential=potential,
                raw_data={"name": project.name, "stars": project.stars},
            )
        else:
            # Hacker News story
            if project.score >= 500:
                potential = "üî• ÁÉ≠Èó®ËØùÈ¢ò"
            elif project.score >= 100:
                potential = "üìà ÂÄºÂæó‰∏ÄËØª"
            else:
                potential = "üå± Êñ∞È≤úËµÑËÆØ"
                
            return ProjectAnalysis(
                title=project.title,
                url=project.url or project.hn_url,
                source="hackernews",
                summary=project.title,
                highlights=[f"üî• {project.score} ÂàÜ", f"üí¨ {project.comments} ËØÑËÆ∫"],
                tech_stack=[],
                target_audience="ÊäÄÊúØÁ§æÂå∫",
                potential=potential,
                raw_data={"title": project.title, "score": project.score},
            )
    
    def _generate_basic_chinese_summary(self, project: GitHubProject) -> str:
        """Generate a Chinese summary for a project using local translation."""
        lang = project.language or "ÂºÄÊ∫ê"
        
        if not project.description:
            return f"‰∏Ä‰∏™ {lang} È°πÁõÆÔºå‚≠ê {project.stars:,}Ôºå‰ªäÊó• +{project.stars_today}"
        
        description = project.description
        
        # Check if description is already Chinese (contains CJK characters)
        def contains_chinese(text):
            return any('\u4e00' <= char <= '\u9fff' for char in text)
        
        if contains_chinese(description):
            return f"[{lang}] {description}"
        
        # Try local translation
        try:
            import translators as ts
            translated = ts.translate_text(
                description[:200],  # Limit length for speed
                translator='bing',  # Use Bing (fast and reliable)
                from_language='en',
                to_language='zh-CN'
            )
            if translated:
                return f"[{lang}] {translated}"
        except Exception as e:
            print(f"  ‚ö†Ô∏è Translation failed: {e}")
        
        # Ultimate fallback: English with language tag
        return f"[{lang}] {description}"
